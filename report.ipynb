{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacettepe University\n",
    "Computer Science and Engineering Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name and Surname : Meltem Tokgöz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identity Number  : 21527381"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course           : BBM 409 Machine Learning Laboratoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment       : Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prog. Language   : Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-mail           : b21527381@cs.hacettepe.edu.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*******************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbor Classification\n",
    "\n",
    "1. Let k-NN(S) be the k Nearest Neighbor classification algorithm on sample set S, which takes the majority of the closest k points where there are 2 classes (positive and negative).\n",
    "\n",
    "• Show that if in both 1-NN(S1) and 1-NN(S2) the label of point x is positive,\n",
    "then in 1-NN(S1 ∪ S2) the label of x is positive.\n",
    "\n",
    "Solution:\n",
    "\n",
    "This is 1-knn so;\n",
    "A point from the S1 set,  eg A is always positive\n",
    "B point from the S2 set,  eg B is always positive\n",
    "\n",
    "Then (S1 ∪ S2) means to combine two sets closest 1 point.One point we receive here is that eg. C is always positive.\n",
    "Because (C == S1 | C == S2 ) is true and s1 is positive s2 is positive so c is postive.\n",
    "\n",
    "• Show an example such that in both 3-NN(S1) and 3-NN(S2) the label of x is\n",
    "positive, and in 3-NN(S1 ∪ S2) the label of x is negative.\n",
    "\n",
    "Solution:\n",
    "\n",
    "This is 3-knn so;\n",
    "The three closest points we get from S1 are as follows:\n",
    "X-positive Y-positive Z-negative\n",
    "\n",
    "The three closest points we get from S2 are as follows:\n",
    "A-positive B-positive C-negative\n",
    "\n",
    "So 3-NN(S1 ∪ S2) means to combine two sets closest 3 points.2 negative 1 positive.Therefore\n",
    "label is negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. One of the problems with k-nearest neighbor learning is selecting a value for k. Say you are given the following data set. This is a binary classification task in which the instances are described by two real-valued attributes.\n",
    "\n",
    "• What value of k minimizes training set error for this data set, and what is the resulting training set error? Why is training set error not a reasonable estimate of test set error, especially given this value of k?\n",
    "\n",
    "Solution:\n",
    "\n",
    "1-knn to minimize error.\n",
    "For self choosing for closest point(overfitting problem)\n",
    "\n",
    "• What value of k minimizes the leave-one-out cross-validation error for this\n",
    "data set, and what is the resulting error? Why is cross-validation a better\n",
    "measure of test set performance?\n",
    "\n",
    "Solution:\n",
    "\n",
    "k=3 minimize error\n",
    "\n",
    "In the K-Fold method, do we still hold out a test set for the very end, and only use the remaining data for training and hyperparameter tuning (ie. we split the remaining data into k folds, and then use the average accuracy after training with each fold (or whatever performance metric we choose) to tune our hyperparameters)\n",
    "The motivation to use cross validation techniques is that when we fit a model, we are fitting it to a training dataset. Without cross validation we only have information on how does our model perform to our in-sample data. Ideally we would like to see how does the model perform when we have a new data in terms of accuracy of its predictions. \n",
    "\n",
    "• Why might using too large values k be bad in this dataset? Why might too\n",
    "small values of k also be bad?\n",
    "\n",
    "Solution:\n",
    "\n",
    "k is too big so underfitting problem.\n",
    "k is too small so overfitting problem.\n",
    "\n",
    "\n",
    "• Sketch the 1-nearest neighbor decision boundary for this dataset.\n",
    "\n",
    "Solution:\n",
    "\n",
    "add image \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "\n",
    "1. Suppose you have m=23 training examples with n=5 features (excluding the additional all-ones feature for the bias term, which you should add). The closed form solution is θ = (XT X)−1XT y. For the given values of m and n, what are the dimensions of X, y, θ in this equation?\n",
    "\n",
    "Solution:\n",
    "\n",
    "dim(x) = 23x6\n",
    "dim(y) = 23x1\n",
    "dim(θ) = 6x1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2. Suppose you have m=50 training examples which are represented with n=200.000 dimensional feature vectors.You want to use multivariate linear regression to fit paremeters θ to our data.Should you prefer gradient descent or the closed form solution?\n",
    "\n",
    "Solution:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3. Which of the following are valid reasons for using feature scaling?\n",
    "(a) It speeds up solving for θ using the normal equation.\n",
    "(b) It prevents the matrix XT X (used in the normal equation) from being noninvertable\n",
    "(singular/degenerate).\n",
    "(c) It speeds up gradient descent by making it require fewer iterations to get to\n",
    "a good solution.\n",
    "(d) It is necessary to prevent gradient descent from getting stuck in local optima.\n",
    "\n",
    "\n",
    "Solution:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*******************************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "PART II: Book Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am done implement a KNN algorithm to find sets of similar users based on common book ratings, and make predictions using the average rating of top-k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I used to import the libraries I used. I did not use the scikit-learn library used for machine learning because it was forbidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#**First-Step**Making imports******\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I read the 4 dataset (book, user, rating and test data) provided with the pandas library. I dropped the properties I didn't use in these data.(For example 'title','year', 'publisher' and etc.)And I created the data frame.I gave the  appropriate name to  data frame columns.When I was processing user data, I just pulled users from canada and usa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python35\\site-packages\\IPython\\core\\interactiveshell.py:3018: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book : \n",
      "(271360, 1)\n",
      "         ISBN\n",
      "0  0195153448\n",
      "1  0002005018\n",
      "2  0060973129\n",
      "3  0374157065\n",
      "4  0393045218\n",
      "All User Size : (278858, 3)\n",
      "Just Usa and Canada Users Size : (161633, 1)\n",
      "User : \n",
      "   userID                            location   age\n",
      "0       1                  nyc, new york, usa   NaN\n",
      "1       2           stockton, california, usa  18.0\n",
      "2       3     moscow, yukon territory, russia   NaN\n",
      "3       4           porto, v.n.gaia, portugal  17.0\n",
      "4       5  farnborough, hants, united kingdom   NaN\n",
      "Rating : \n",
      "(629144, 3)\n",
      "   userID                            location   age\n",
      "0       1                  nyc, new york, usa   NaN\n",
      "1       2           stockton, california, usa  18.0\n",
      "2       3     moscow, yukon territory, russia   NaN\n",
      "3       4           porto, v.n.gaia, portugal  17.0\n",
      "4       5  farnborough, hants, united kingdom   NaN\n",
      "Test: \n",
      "(419431, 3)\n",
      "   userID                            location   age\n",
      "0       1                  nyc, new york, usa   NaN\n",
      "1       2           stockton, california, usa  18.0\n",
      "2       3     moscow, yukon territory, russia   NaN\n",
      "3       4           porto, v.n.gaia, portugal  17.0\n",
      "4       5  farnborough, hants, united kingdom   NaN\n"
     ]
    }
   ],
   "source": [
    "#******************Read Data Set***********************************************\n",
    "\n",
    "#for book dataset\n",
    "\n",
    "book = pd.read_csv('BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "book.columns = ['ISBN', 'title', 'author', 'year', 'publisher', 'UrlS', 'UrlM', 'UrlL']\n",
    "unnec1 = ['title','year', 'publisher', 'author', 'UrlS', 'UrlM', 'UrlL']\n",
    "book = book.drop(unnec1, axis=1)\n",
    "print(\"Book : \")\n",
    "print(book.shape)\n",
    "print(book.head(5))\n",
    "\n",
    "#for user dataset\n",
    "\n",
    "user = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "user.columns = ['userID', 'location', 'age']\n",
    "print('All User Size :',user.shape)\n",
    "new_user = user[user['location'].str.contains(\"usa|canada\")]\n",
    "unnec2 = ['age','location']\n",
    "new_user = new_user.drop(unnec2,axis=1)\n",
    "print('Just Usa and Canada Users Size :',new_user.shape)\n",
    "print('User : ')\n",
    "print(user.head(5))\n",
    "\n",
    "#for rating dataset\n",
    "\n",
    "rating = pd.read_csv('BX-Book-Ratings-Train.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "rating.columns = ['userID', 'ISBN', 'bookRating']\n",
    "print('Rating : ')\n",
    "print(rating.shape)\n",
    "print(user.head(5))\n",
    "\n",
    "#for test dataset\n",
    "\n",
    "test = pd.read_csv('BXBookRatingsTest.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "test.columns = ['userID', 'ISBN', 'bookRating']\n",
    "print('Test: ')\n",
    "print(test.shape)\n",
    "print(user.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I merged book and rating datas as requested.Then, I have combined this data with the user dataset.In this way, I have reached the data set which is in usa and canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_data size:  (67300, 3)\n",
      "   userID        ISBN  bookRating\n",
      "0   52584  080411109X           9\n",
      "1   52584  044021145X           0\n",
      "2   52584  068484477X           0\n",
      "3   52584  015100692X           0\n",
      "4   52584  044661162X           0\n"
     ]
    }
   ],
   "source": [
    "#********************Combine Data Set******************************************\n",
    "\n",
    "comBookRate = pd.merge(rating, book, on='ISBN')\n",
    "new_data = comBookRate.merge(new_user, on = 'userID')\n",
    "print(\"new_data size: \",new_data.shape) #same pdf data number (66961) \n",
    "print(new_data.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I divided the data into two parts as train and validation. I did this randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (53840, 3)\n",
      "       userID        ISBN  bookRating\n",
      "23734  278418  185326072X           0\n",
      "9208   255489  044900399X           0\n",
      "55238    5490  156383121X           5\n",
      "25969  234755  055329461X           4\n",
      "25588   73394  082177400X           0\n",
      "Validation : (13460, 3)\n",
      "    userID        ISBN  bookRating\n",
      "2    52584  068484477X           0\n",
      "3    52584  015100692X           0\n",
      "4    52584  044661162X           0\n",
      "5    52584  042513699X           0\n",
      "17   52584  1558532161           0\n"
     ]
    }
   ],
   "source": [
    "#******************Divede data set (train and validation)**********************\n",
    "\n",
    "train = new_data.sample(frac=0.8,random_state=200)\n",
    "validation = new_data.drop(train.index)\n",
    "print('Train :',train.shape)\n",
    "print(train.head(5))\n",
    "print('Validation :',validation.shape)\n",
    "print(validation.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm building a matrix from the train dataframe.I fill the empty ratings 0 with the matrix.I'm doing this because it's easy to list all the votes while calculating similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_matrix : (13853, 24242)\n",
      "ISBN    000104799X  000123207X  000224408X  000224554X  000225056X  \\\n",
      "userID                                                               \n",
      "8                0           0           0           0           0   \n",
      "32               0           0           0           0           0   \n",
      "\n",
      "ISBN    000225218X  000225414X  000225851X  000231780X  000250653X  \\\n",
      "userID                                                               \n",
      "8                0           0           0           0           0   \n",
      "32               0           0           0           0           0   \n",
      "\n",
      "ISBN       ...      B0000VZEH8  B00011SOXI  B00013AFZQ  B00016560C  \\\n",
      "userID     ...                                                       \n",
      "8          ...               0           0           0           0   \n",
      "32         ...               0           0           0           0   \n",
      "\n",
      "ISBN    B0001FZGBC  B0001FZGRQ  B0001FZGTO  B0001GDNCK  B0001GMSV2  B0001I1KOG  \n",
      "userID                                                                          \n",
      "8                0           0           0           0           0           0  \n",
      "32               0           0           0           0           0           0  \n",
      "\n",
      "[2 rows x 24242 columns]\n"
     ]
    }
   ],
   "source": [
    "#*******************Create Matrix**********************************************\n",
    "\n",
    "train_matrix = train.pivot(index='userID',columns='ISBN',values='bookRating').fillna(0)\n",
    "train_matrix = train_matrix.astype(np.int32)\n",
    "print('train_matrix :',train_matrix.shape)\n",
    "print(train_matrix.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have 3 similarity implementations that you want from us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*******************Now Calculating Similarities*******************************\n",
    "\n",
    "def sim_cosine (a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def euclidian_distance(a,b):\n",
    "    return norm(np.array(b)-np.array(a))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a similarity finding function.I'm sending the train_matrix data as a parameter to be able to calculate similarity with other users.And I am sending test user as a parameter.This function first finds the user given to me for the test from the train data.Because I have to get information about this user from the train_matrix data and teach them to the machine.In this way, I will be user based cf.Then, I am making similarity calculations with each other in the train_matrix data.So I find the similarity of each with the user in the test user and add it to the dictionary(dict_users_sim is a nested dictionary).\n",
    "(dict_users_sim format is: {test_userID : {train_userID : similarity }, ...} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*********************Find similarity************************************\n",
    "dict_users_sim ={}\n",
    "\n",
    "def findSimilarity(user,train):\n",
    "    dict_users_sim[user] = {}\n",
    "    for v in range(len(train)):\n",
    "        if train.index[v] == user: #find test user for train_matrix\n",
    "            for t in range(len(train)):\n",
    "                if not train.index[v] == train.index[t]:\n",
    "                    sim = sim_cosine(train.values[v],train.values[t]) #calculate similarity each user in train_matrix \n",
    "                    if not math.isnan(sim):\n",
    "                        if not sim == 0:\n",
    "                            dict_users_sim[train.index[v]][train.index[t]] = sim #add dictionary\n",
    "    #print(dict_users_sim[user])\n",
    "    return dict_users_sim[user] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding all the similarities, I wrote a function to find the highest k similarity.As a parameter to this function, I find the similarities, the user and the k value I have found before.I find the highest k similarity and add it to the dict_max_sim dictionary.\n",
    "(dict_max_sim is a nested dictionary)\n",
    "(dict_max_sim format is: {test_userID : {train_userID : max_similarity }, ...} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*********************Find max k-similarity************************************     \n",
    "dict_max_sim ={}\n",
    "\n",
    "def findMaxSim(dict_users_sim,user,k): \n",
    "    dict_max_sim[user] ={}\n",
    "    c = Counter(dict_users_sim)\n",
    "    mc = c.most_common(k)\n",
    "    for i in range(len(mc)):\n",
    "            dict_max_sim[user][mc[i][0]] = mc[i][1]\n",
    "    print(dict_max_sim[user])\n",
    "    return dict_max_sim[user] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have the prediction with the maximum resolution I found.I do this in two ways.1-Normal KNN 2-Weighted KNN.\n",
    "In order to make weighted knn, I have to find the weight of the highest k similarity I found before.\n",
    "I'm writing the following function to find the weights.\n",
    "I measured the Euclidean distance for weight.I have calculated the weight for the highest k user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*********************Find max k-similarity weight*****************************\n",
    "\n",
    "dict_weight_sim ={}\n",
    "def max_sim_weight(dict_max_sim,user,train):\n",
    "    dict_weight_sim[user] = {}\n",
    "    for v in range(len(train)):\n",
    "        if train.index[v] == user: #find test user for train_matrix\n",
    "            for m in dict_max_sim:\n",
    "                for t in range(len(train)):\n",
    "                    if train.index[t] == m: #calculate weight find max sim user in train_matrix \n",
    "                        weight = euclidian_distance(train.values[v],train.values[t])\n",
    "                        dict_weight_sim[train.index[v]][train.index[t]] = weight #add dictionary\n",
    "    #print(dict_weight_sim[user])\n",
    "    return dict_weight_sim[user]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am doing  a prediction function.\n",
    "Like I said, I did this as weighted knn and normal knn.I find ratings for the k max similarity I found here with train_matrix.\n",
    "I explained what inside the code means  to this functions (with comment)(note: * print means predict book don't exist train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*********************Prediction Function**************************************\n",
    "\n",
    "def normalPredict(train,maxSim,user,book,k):\n",
    "    vote_absent = 0\n",
    "    sum_perdict = 0\n",
    "    for m in maxSim: #max k similarity users\n",
    "        if not book in train.columns: #predict book not exist train so rating equal 0\n",
    "            print('*',m, ' ',book ,' 0',)\n",
    "        else:\n",
    "            if train.loc[m, book] == 0: #predict book exist train but rating is 0 so not rating \n",
    "                print(m, ' ',book ,' ',train.loc[m, book])\n",
    "                vote_absent = vote_absent +1\n",
    "            else:\n",
    "                print(m, ' ',book ,' ',train.loc[m, book]) #predict book exist and rate exist \n",
    "                sum_perdict = sum_perdict + train.loc[m, book]\n",
    "    print(vote_absent) #Not rating book (0)\n",
    "    result = sum_perdict / (len(maxSim)) #total rate / k \n",
    "    print(result) \n",
    "    return result\n",
    "\n",
    "def weightedPredict(train,maxSim,user,book,k,weightUser):\n",
    "    sum_perdict = 0\n",
    "    vote_absent = 0\n",
    "    for m in maxSim:\n",
    "        if not book in train.columns: #predict book not exist train so rating equal 0\n",
    "            print('*',m, ' ',book ,' 0',)\n",
    "        else:\n",
    "            if train.loc[m, book] == 0: #predict book exist train but rating is 0 so not rating \n",
    "                vote_absent = vote_absent +1\n",
    "            else:\n",
    "                print(train.loc[m, book])\n",
    "                sum_perdict = sum_perdict + (weightUser[m]*train.loc[m, book]) #Unlike normal knn, I multiplication the\n",
    "                                                                            #votes into weight and then divide by total weight.\n",
    "    result = sum_perdict / sum(weightUser)\n",
    "    #print(result) \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section, I find error for predict book with (real rating - Predict Rating).And return error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#*********************Now Calculating Error************************************\n",
    "def findError(predict_rate,real_rate):\n",
    "    error = real_rate- predict_rate\n",
    "    print('Knn error rate :',error)\n",
    "    return error\n",
    "    \n",
    "def findErrorWeightedKnn(predict_rate,real_rate):\n",
    "    error = real_rate- predict_rate\n",
    "    print('Weighted Knn error rate :',error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#important notes***************************************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally,I call all the functions I do on the test data in this section.if the predict_user already exists in the dictionary I'm using it.If not, I call the functions for him and find the necessary values.To try the validation, just type validation instead of the test.For this assignment, I think the most accurate k value is 3.Because I tried other values.Because there are not more than 3 values of similarity is not enough as most of the time.K When I received a smaller value, my accuracy fell.\n",
    "\n",
    "I couldn't run my test on the test data because the program was very slow.However I saw mae run on a small section.\n",
    "I tried small section I saw mae 0.3 \n",
    "Although my teacher generally I understood the logic of homework, knn and I am doing it.But I could not see mae for all the test data due to the size of the data set and the slowness of the program.I think the codes are correct, or you can want to test it at an interval such as test [: 20].\n",
    "I've also left print lines as comments in code, but you can see the results of all the work I've done with print line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict : userID : 253871 Book ISBN :  1551668696 Book Rating : 9\n",
      "{40136: 0.5255883312276368, 127113: 0.5255883312276367, 151043: 0.5255883312276367}\n",
      "40136   1551668696   0\n",
      "127113   1551668696   0\n",
      "151043   1551668696   0\n",
      "3\n",
      "0.0\n",
      "Knn error rate : 9.0\n",
      "Knn error rate : 9.0\n",
      "Predict : userID : 82164 Book ISBN :  038078260X Book Rating : 0\n",
      "{204808: 1.0, 50130: 1.0, 168719: 1.0}\n",
      "* 204808   038078260X  0\n",
      "* 50130   038078260X  0\n",
      "* 168719   038078260X  0\n",
      "0\n",
      "0.0\n",
      "* 204808   038078260X  0\n",
      "* 50130   038078260X  0\n",
      "* 168719   038078260X  0\n",
      "Knn error rate : 0.0\n",
      "Knn error rate : 0.0\n",
      "Predict : userID : 171999 Book ISBN :  60928336 Book Rating : 0\n",
      "{}\n",
      "Knn error rate : 0\n",
      "Knn error rate : 0\n",
      "mean absolute error normal knn:  3.0\n",
      "mean absolute error for weighted knn:  3.0\n"
     ]
    }
   ],
   "source": [
    "#***********************For the test or validation file******************************************\n",
    "sum_error = 0\n",
    "sum_weighted_error = 0 \n",
    "for i in range(len(test[:3])): #delete validation write test for test file \n",
    "    user = test.iloc[i]['userID']\n",
    "    book = test.iloc[i]['ISBN']\n",
    "    rate = test.iloc[i]['bookRating']\n",
    "    k = 3\n",
    "    print('Predict :','userID :',user,'Book ISBN : ',book,'Book Rating :',rate)\n",
    "    if user in dict_users_sim.keys():\n",
    "        if user in dict_max_sim.keys():\n",
    "            if not bool(dict_max_sim[user]):\n",
    "                sum_error = sum_error + findError(0,rate)\n",
    "                sum_weighted_error = sum_weighted_error + findError(0,rate)\n",
    "            else:\n",
    "                pre_rate1 = normalPredict(train_matrix, dict_max_sim[user],user,book,k)\n",
    "                a = max_sim_weight(dict_max_sim[user],user,train_matrix)\n",
    "                prew_rate1 = weightedPredict(train_matrix, dict_max_sim[user],user,book,k,a)\n",
    "                sum_error = sum_error + findError(pre_rate1,rate)\n",
    "                sum_weighted_error = sum_weighted_error + findError(prew_rate1,rate)\n",
    "        else:\n",
    "            a = findMaxSim(dict_users_sim[user],user,k)\n",
    "            b = max_sim_weight(a,user,train_matrix)\n",
    "            if not bool(a):\n",
    "                sum_error = sum_error + findError(0,rate)\n",
    "                sum_weighted_error = sum_weighted_error + findError(0,rate)\n",
    "            else: \n",
    "                pre_rate2 = normalPredict(train_matrix,a,user,book,k) \n",
    "                prew_rate2 = weightedPredict(train_matrix,a,user,book,k,b)\n",
    "                sum_error = sum_error + findError(pre_rate2,rate)\n",
    "                sum_weighted_error = sum_weighted_error + findError(prew_rate2,rate)\n",
    "          \n",
    "    else: \n",
    "        b = findSimilarity(user,train_matrix)\n",
    "        c = findMaxSim(b,user,k)\n",
    "        d = max_sim_weight(c,user,train_matrix)\n",
    "        if not bool(c):\n",
    "                sum_error = sum_error + findError(0,rate)\n",
    "                sum_weighted_error = sum_weighted_error + findError(0,rate)\n",
    "        else: \n",
    "            pre_rate3 = normalPredict(train_matrix,c,user,book,k)  \n",
    "            prew_rate3 = weightedPredict(train_matrix,c,user,book,k,d)\n",
    "            sum_error = sum_error + findError(pre_rate3,rate)\n",
    "            sum_weighted_error = sum_weighted_error + findError(prew_rate3,rate)\n",
    "        \n",
    "sum_n = len(test[:3])\n",
    "mae_error  = sum_error /  sum_n\n",
    "print('mean absolute error normal knn: ',mae_error)\n",
    "\n",
    "mae_weighted_error = sum_weighted_error / sum_n\n",
    "print('mean absolute error for weighted knn: ',mae_weighted_error)\n",
    "      \n",
    "#***********************************END****************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
